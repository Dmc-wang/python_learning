"""
è±†ç“£ç”µå½±æ•°æ®æ¸…æ´—æ¨¡å—
æ”¯æŒç±»å‹è½¬æ¢ã€ç¼ºå¤±å€¼å¤„ç†ã€å¼‚å¸¸å€¼æ£€æµ‹ã€æ•°æ®è´¨é‡æŠ¥å‘Š
"""

import json
import logging
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple

import numpy as np
import pandas as pd

# from .fetcher import Movie


@dataclass
class CleanerConfig:
    """æ¸…æ´—é…ç½®ç±»"""
    # è¾“å…¥è¾“å‡ºè·¯å¾„
    input_dir: Path = Path("data/raw")
    output_dir: Path = Path("data/processed")
    
    # æ•°æ®ç±»å‹æ˜ å°„
    dtype_mapping: Dict[str, str] = field(default_factory=lambda: {
        'rank': 'Int16',           # æ’åï¼ˆå¯ä¸ºç©ºï¼‰
        'title': 'string',         # æ ‡é¢˜ï¼ˆä¸å¯ä¸ºç©ºï¼‰
        'rating': 'float32',       # è¯„åˆ†
        'rating_count': 'Int64',   # è¯„ä»·äººæ•°
        'quote': 'string',         # ç®€ä»‹ï¼ˆå¯ä¸ºç©ºï¼‰
        'year': 'Int16',           # å¹´ä»½
        'director': 'string',      # å¯¼æ¼”
        'actors': 'string',        # æ¼”å‘˜ï¼ˆå¯ä¸ºç©ºï¼‰
        'genres': 'string',        # ç±»å‹
        'duration': 'string',      # æ—¶é•¿ï¼ˆå¯ä¸ºç©ºï¼‰
    })
    
    # æ¸…æ´—è§„åˆ™
    min_rating: float = 8.0      # æœ€ä½è¯„åˆ†é˜ˆå€¼
    min_rating_count: int = 5000  # æœ€ä½è¯„ä»·äººæ•°
    drop_duplicates_by: List[str] = field(default_factory=lambda: ['title', 'year'])
    
    # ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥
    fill_na_values: Dict[str, Any] = field(default_factory=lambda: {
        'quote': 'æš‚æ— ç®€ä»‹',
        'actors': 'æš‚æ— æ¼”å‘˜ä¿¡æ¯',
        'duration': 'æœªçŸ¥',
    })


class DataCleaner:
    """æ•°æ®æ¸…æ´—å™¨ä¸»ç±»"""
    
    def __init__(self, config: Optional[CleanerConfig] = None):
        self.config = config or CleanerConfig()
        self.logger = self._setup_logger()
        self.raw_df: Optional[pd.DataFrame] = None
        self.cleaned_df: Optional[pd.DataFrame] = None
        self.report: Dict[str, Any] = {}
    
    def _setup_logger(self) -> logging.Logger:
        """é…ç½®æ—¥å¿—"""
        logger = logging.getLogger(__name__)
        if not logger.handlers:
            logger.setLevel(logging.INFO)
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        return logger
    
    def load_data(self, filename: Optional[str] = None) -> pd.DataFrame:
        """
        åŠ è½½åŸå§‹æ•°æ®
        
        Args:
            filename: CSVæ–‡ä»¶åï¼ŒNoneåˆ™è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ–‡ä»¶
        
        Returns:
            åŸå§‹DataFrame
        """
        if filename:
            input_path = self.config.input_dir / filename
        else:
            # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„CSVæ–‡ä»¶
            csv_files = list(self.config.input_dir.glob("movies_top250_*.csv"))
            if not csv_files:
                raise FileNotFoundError(f"åœ¨ {self.config.input_dir} ä¸­æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
            input_path = max(csv_files, key=lambda p: p.stat().st_mtime)
        
        self.logger.info(f"æ­£åœ¨åŠ è½½æ•°æ®: {input_path}")
        
        # è¯»å–CSVï¼ˆæ³¨æ„ç¼–ç å’Œç¼ºå¤±å€¼è¡¨ç¤ºï¼‰
        df = pd.read_csv(
            input_path,
            dtype=self.config.dtype_mapping,
            na_values=['', 'NULL', 'null', 'N/A', 'NaN']
        )
        
        self.logger.info(f"åŠ è½½å®Œæˆï¼å…±{len(df)}è¡Œï¼Œ{len(df.columns)}åˆ—")
        self.raw_df = df
        return df
    
    def validate_schema(self, df: pd.DataFrame) -> bool:
        """
        éªŒè¯æ•°æ®æ¨¡å¼ï¼ˆSchema Validationï¼‰
        
        Returns:
            æ˜¯å¦é€šè¿‡éªŒè¯
        """
        self.logger.info("å¼€å§‹æ•°æ®æ¨¡å¼éªŒè¯...")
        
        required_columns = list(self.config.dtype_mapping.keys())
        missing_cols = set(required_columns) - set(df.columns)
        
        if missing_cols:
            self.logger.error(f"ç¼ºå¤±å¿…éœ€åˆ—: {missing_cols}")
            return False
        
        # æ£€æŸ¥å…³é”®åˆ—çš„éç©º
        critical_columns = ['title', 'rating', 'rank']
        for col in critical_columns:
            null_count = df[col].isnull().sum()
            if null_count > 0:
                self.logger.warning(f"å…³é”®åˆ— '{col}' æœ‰{null_count}ä¸ªç©ºå€¼")
        
        self.logger.info("æ•°æ®æ¨¡å¼éªŒè¯é€šè¿‡")
        return True
    
    def clean_types(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ•°æ®ç±»å‹è½¬æ¢å’Œè§„èŒƒåŒ–
        """
        self.logger.info("å¼€å§‹ç±»å‹è½¬æ¢...")
        
        # è½¬æ¢ä¸ºé…ç½®ä¸­æŒ‡å®šçš„ç±»å‹
        for col, dtype in self.config.dtype_mapping.items():
            if col in df.columns:
                try:
                    # ç‰¹æ®Šå¤„ç†ï¼šå­—ç¬¦ä¸²ç±»å‹
                    if dtype == 'string':
                        df[col] = df[col].astype('string')
                    else:
                        df[col] = df[col].astype(dtype)
                except Exception as e:
                    self.logger.warning(f"åˆ— '{col}' è½¬æ¢ç±»å‹å¤±è´¥: {e}")
        
        # ç‰¹æ®Šå¤„ç†ï¼šè¯„ä»·äººæ•°ä¸­çš„ç¼ºå¤±å€¼
        if 'rating_count' in df.columns:
            df['rating_count'] = pd.to_numeric(
                df['rating_count'],
                errors='coerce'
            ).astype('Int64')
        
        self.logger.info("ç±»å‹è½¬æ¢å®Œæˆ")
        return df
    
    def handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        å¤„ç†ç¼ºå¤±å€¼
        """
        self.logger.info("å¼€å§‹å¤„ç†ç¼ºå¤±å€¼...")
        
        missing_report = {}
        
        for col in df.columns:
            null_count = df[col].isnull().sum()
            if null_count > 0:
                missing_report[col] = null_count
                self.logger.info(f"åˆ— '{col}' æœ‰{null_count}ä¸ªç¼ºå¤±å€¼({null_count/len(df)*100:.1f}%)")
        
        # æ ¹æ®é…ç½®å¡«å……ç¼ºå¤±å€¼
        for col, fill_value in self.config.fill_na_values.items():
            if col in df.columns:
                df[col] = df[col].fillna(fill_value)
                self.logger.info(f"åˆ— '{col}' å·²å¡«å……ç¼ºå¤±å€¼: {fill_value}")
        
        # åˆ é™¤å…³é”®åˆ—ä»æœ‰ç¼ºå¤±çš„è¡Œ
        critical_columns = ['title', 'rating', 'rank']
        df = df.dropna(subset=critical_columns)
        
        self.logger.info(f"ç¼ºå¤±å€¼å¤„ç†å®Œæˆï¼Œå‰©ä½™{len(df)}è¡Œ")
        self.report['missing_values'] = missing_report
        return df
    
    def remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        å»é‡å¤„ç†
        """
        self.logger.info("å¼€å§‹å»é‡...")
        
        before_count = len(df)
        
        # æ ¹æ®é…ç½®çš„åˆ—å»é‡
        df = df.drop_duplicates(
            subset=self.config.drop_duplicates_by,
            keep='first'
        )
        
        after_count = len(df)
        removed = before_count - after_count
        
        self.logger.info(f"å»é‡å®Œæˆï¼Œåˆ é™¤{removed}æ¡é‡å¤æ•°æ®")
        self.report['duplicates_removed'] = removed
        
        return df
    
    def extract_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ç‰¹å¾å·¥ç¨‹ï¼šä»ç°æœ‰æ•°æ®æå–æ–°ç‰¹å¾
        """
        self.logger.info("å¼€å§‹ç‰¹å¾æå–...")
        
        # æå–å¹´ä»½ï¼ˆå¦‚æœå°šæœªæå–ï¼‰
        if 'year' in df.columns:
            # ç¡®ä¿yearæ˜¯æ•°å€¼ç±»å‹
            df['year'] = pd.to_numeric(df['year'], errors='coerce')
            self.logger.info("å¹´ä»½åˆ—å·²è½¬æ¢ä¸ºæ•°å€¼ç±»å‹")
        
        # åˆ›å»ºè¯„åˆ†ç­‰çº§
        def rating_grade(rating):
            if pd.isna(rating):
                return 'æœªçŸ¥'
            elif rating >= 9.5:
                return 'ç¥ä½œ'
            elif rating >= 9.0:
                return 'ç»å…¸'
            elif rating >= 8.5:
                return 'ä½³ä½œ'
            elif rating >= 8.0:
                return 'ä¼˜ç§€'
            else:
                return 'è‰¯å¥½'
        
        df['rating_grade'] = df['rating'].apply(rating_grade)
        self.logger.info("æ–°å¢åˆ—: rating_gradeï¼ˆè¯„åˆ†ç­‰çº§ï¼‰")
        
        # è®¡ç®—è¯„ä»·äººæ•°çš„å¯¹æ•°ï¼ˆç”¨äºåç»­åˆ†æï¼‰
        if 'rating_count' in df.columns:
            df['log_rating_count'] = np.log10(df['rating_count'].replace(0, 1))
            self.logger.info("æ–°å¢åˆ—: log_rating_countï¼ˆè¯„ä»·äººæ•°å¯¹æ•°ï¼‰")
        
        # è§£ædurationä¸ºåˆ†é’Ÿæ•°
        def parse_duration(duration_str):
            if pd.isna(duration_str) or duration_str == 'æœªçŸ¥':
                return None
            try:
                # æ ¼å¼: "142åˆ†é’Ÿ" æˆ– "142åˆ†é’Ÿ / å¯¼æ¼”å‰ªè¾‘ç‰ˆ"
                import re
                match = re.search(r'(\d+)\s*åˆ†é’Ÿ', str(duration_str))
                if match:
                    return int(match.group(1))
            except:
                pass
            return None
        
        df['duration_minutes'] = df['duration'].apply(parse_duration)
        df['duration_minutes'] = df['duration_minutes'].astype('Int16')
        self.logger.info("æ–°å¢åˆ—: duration_minutesï¼ˆæ—¶é•¿åˆ†é’Ÿæ•°ï¼‰")
        
        return df
    
    def filter_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ•°æ®è¿‡æ»¤ï¼šåˆ é™¤ä¸ç¬¦åˆæ¡ä»¶çš„æ•°æ®
        """
        self.logger.info("å¼€å§‹æ•°æ®è¿‡æ»¤...")
        
        before_count = len(df)
        
        # è¯„åˆ†è¿‡æ»¤
        if 'rating' in df.columns:
            df = df[df['rating'] >= self.config.min_rating]
            self.logger.info(f"åº”ç”¨è¯„åˆ†é˜ˆå€¼ â‰¥{self.config.min_rating}")
        
        # è¯„ä»·äººæ•°è¿‡æ»¤
        if 'rating_count' in df.columns:
            df = df[df['rating_count'] >= self.config.min_rating_count]
            self.logger.info(f"åº”ç”¨è¯„ä»·äººæ•°é˜ˆå€¼ â‰¥{self.config.min_rating_count}")
        
        after_count = len(df)
        filtered = before_count - after_count
        
        self.logger.info(f"è¿‡æ»¤å®Œæˆï¼Œåˆ é™¤{filtered}æ¡ä¸ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
        self.report['filtered_out'] = filtered
        
        return df
    
    def validate_data_quality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        æ•°æ®è´¨é‡æ£€æŸ¥
        """
        self.logger.info("å¼€å§‹æ•°æ®è´¨é‡æ£€æŸ¥...")
        
        quality_report = {
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'null_counts': df.isnull().sum().to_dict(),
            'duplicates': df.duplicated().sum(),
            'rating_stats': {
                'min': df['rating'].min(),
                'max': df['rating'].max(),
                'mean': df['rating'].mean(),
                'median': df['rating'].median(),
            } if 'rating' in df.columns else {},
            'year_range': {
                'min': df['year'].min(),
                'max': df['year'].max(),
            } if 'year' in df.columns else {},
        }
        
        # æ£€æŸ¥å¼‚å¸¸å€¼
        if 'rating' in df.columns:
            outliers = df[(df['rating'] < 8.0) | (df['rating'] > 10.0)]
            quality_report['rating_outliers'] = len(outliers)
        
        if 'year' in df.columns:
            year_outliers = df[(df['year'] < 1900) | (df['year'] > 2030)]
            quality_report['year_outliers'] = len(year_outliers)
        
        self.logger.info("æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆ")
        return quality_report
    
    def clean(self, input_file: Optional[str] = None) -> pd.DataFrame:
        """
        æ‰§è¡Œå®Œæ•´æ¸…æ´—æµç¨‹
        
        Returns:
            æ¸…æ´—åçš„DataFrame
        """
        self.logger.info("=" * 50)
        self.logger.info("å¼€å§‹æ•°æ®æ¸…æ´—æµç¨‹...")
        
        # 1. åŠ è½½æ•°æ®
        df = self.load_data(input_file)
        
        # 2. éªŒè¯æ¨¡å¼
        if not self.validate_schema(df):
            raise ValueError("æ•°æ®æ¨¡å¼éªŒè¯å¤±è´¥")
        
        # 3. ç±»å‹è½¬æ¢
        df = self.clean_types(df)
        
        # 4. å¤„ç†ç¼ºå¤±å€¼
        df = self.handle_missing_values(df)
        
        # 5. å»é‡
        df = self.remove_duplicates(df)
        
        # 6. ç‰¹å¾å·¥ç¨‹
        df = self.extract_features(df)
        
        # 7. æ•°æ®è¿‡æ»¤
        df = self.filter_data(df)
        
        # 8. è´¨é‡æ£€æŸ¥
        self.report['quality'] = self.validate_data_quality(df)
        
        self.cleaned_df = df
        self.logger.info("æ•°æ®æ¸…æ´—æµç¨‹å®Œæˆï¼")
        self.logger.info("=" * 50)
        
        return df
    
    def save_cleaned_data(self, df: Optional[pd.DataFrame] = None, 
                         output_format: str = 'csv') -> Path:
        """
        ä¿å­˜æ¸…æ´—åçš„æ•°æ®
        
        Args:
            df: DataFrameï¼ŒNoneåˆ™ä½¿ç”¨self.cleaned_df
            output_format: è¾“å‡ºæ ¼å¼ ('csv', 'json', 'excel')
        
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if df is None:
            df = self.cleaned_df
        
        if df is None:
            raise ValueError("æ²¡æœ‰å¯ä¿å­˜çš„æ•°æ®")
        
        self.config.output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
        
        if output_format == 'csv':
            output_path = self.config.output_dir / f"cleaned_movies_{timestamp}.csv"
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
        elif output_format == 'json':
            output_path = self.config.output_dir / f"cleaned_movies_{timestamp}.json"
            df.to_json(output_path, orient='records', force_ascii=False, indent=2)
        elif output_format == 'excel':
            output_path = self.config.output_dir / f"cleaned_movies_{timestamp}.xlsx"
            df.to_excel(output_path, index=False, engine='openpyxl')
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {output_format}")
        
        self.logger.info(f"æ¸…æ´—æ•°æ®å·²ä¿å­˜: {output_path}")
        return output_path
    
    def save_report(self, report: Optional[Dict[str, Any]] = None) -> Path:
        """
        ä¿å­˜æ¸…æ´—æŠ¥å‘Š
        """
        if report is None:
            report = self.report
        
        self.config.output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
        report_path = self.config.output_dir / f"cleaning_report_{timestamp}.json"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        self.logger.info(f"æ¸…æ´—æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report_path


# ==================== å‘½ä»¤è¡Œæ¥å£ ====================

def main():
    """ä¸»å…¥å£å‡½æ•°"""
    logging.basicConfig(level=logging.INFO)
    
    cleaner = DataCleaner()
    
    try:
        # æ‰§è¡Œæ¸…æ´—
        cleaned_df = cleaner.clean()
        
        # ä¿å­˜æ•°æ®
        output_path = cleaner.save_cleaned_data(cleaned_df, output_format='csv')
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = cleaner.save_report()
        
        print("\n" + "=" * 50)
        print("âœ… æ•°æ®æ¸…æ´—å®Œæˆï¼")
        print(f"æ¸…æ´—åæ•°æ®: {len(cleaned_df)}è¡Œ Ã— {len(cleaned_df.columns)}åˆ—")
        print(f"æ•°æ®æ–‡ä»¶: {output_path}")
        print(f"æŠ¥å‘Šæ–‡ä»¶: {report_path}")
        print("=" * 50)
        
        # é¢„è§ˆæ•°æ®
        print("\nğŸ“Š é¢„è§ˆå‰5è¡Œ:")
        print(cleaned_df.head())
        
        # æ˜¾ç¤ºè´¨é‡æŠ¥å‘Šæ‘˜è¦
        print("\nğŸ“ˆ è´¨é‡æŠ¥å‘Šæ‘˜è¦:")
        print(f"  - åˆ é™¤é‡å¤æ•°æ®: {cleaner.report.get('duplicates_removed', 0)}æ¡")
        print(f"  - è¿‡æ»¤æ•°æ®: {cleaner.report.get('filtered_out', 0)}æ¡")
        
        quality = cleaner.report.get('quality', {})
        if 'rating_stats' in quality:
            stats = quality['rating_stats']
            print(f"  - è¯„åˆ†èŒƒå›´: {stats['min']:.1f} - {stats['max']:.1f}")
            print(f"  - å¹³å‡è¯„åˆ†: {stats['mean']:.2f}")
        
    except Exception as e:
        print(f"âŒ æ¸…æ´—å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()